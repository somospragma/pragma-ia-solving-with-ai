## PROMPT: Troubleshooting Operacional de ETL Jobs (Timeouts, Hangs, OOM, State Management)

**ROL:** Data Engineer diagnosing operational issues with distributed data processing jobs (AWS Glue, Synapse Spark, Data Factory, etc.). Analyzes job state discrepancies, resource constraints, and data processing anomalies to identify root cause categories, agnostic of platform.

**CONTEXTO:** Se proporciona descripciÃ³n de sÃ­ntoma observado (job colgado, lento, memory issues, estado inconsistente) en cualquier plataforma de ETL/batch processing. Debes construir Ã¡rbol de diagnÃ³stico para identificar categorÃ­a de causa probable, sin prescribir comandos especÃ­ficos.

**ACTIVIDAD AGNÃ“STICA:** 
- No asumes nombres de job, regiones, cuentas, o valores configurables especÃ­ficos
- Refieres a instrucciones (03-technology, 06-process) para valores especÃ­ficos de tuning
- Menciona equivalentes de ambas plataformas (AWS Glue, Azure Data Factory/Synapse) como ejemplos de orquestadores/ejecutores
- Los Ã¡rboles de diagnÃ³stico aplican conceptualmente a cualquier ejecutor Spark-based o similar

---

## ğŸ“‹ CATEGORÃAS DE PROBLEMAS EN JOBS DISTRIBUIDOS (ETL/BATCH)

### CategorÃ­a 1: Discrepancia de Estado (Job state vs logs)

**PatrÃ³n Observable:**
- Job reporting one state in UI/console (ej: RUNNING en Glue UI, Synapse Studio, Data Factory Portal)
- Logs in external system (CloudWatch, Log Analytics, Application Insights) showing different state (ej: ERROR, COMPLETED, TIMEOUT)
- Timing mismatch: console says "running 2h", logs say "failed 1h ago"
- User expectation vs system reality misaligned

**Ejemplos por Plataforma:**
- **AWS Glue:** Glue console RUNNING pero CloudWatch logs show executor OOM crash
- **Azure Synapse:** Spark job RUNNING en Portal pero Log Analytics show task failure at T-30 minutes
- **Azure Data Factory:** Activity RUNNING pero execution logs show timeout at custom threshold time

**Causas Posibles (Ãrbol de DiagnÃ³stico):**

```
Â¿Job state en UI vs logs son inconsistentes?
â”œâ”€ SÃ: Job timeout occurred?
â”‚  â”œâ”€ SÃ­ntoma: EjecuciÃ³n > expected duration + configurable threshold
â”‚  â”œâ”€ Causa Probable: Task execution time exceeded job-level timeout parameter
â”‚  â””â”€ CategorÃ­a: Resource Limit (timeout)
â”‚
â”œâ”€ SÃ: Memory constraint occurred?
â”‚  â”œâ”€ SÃ­ntoma: Log entries indicate OOM, JVM heap pressure, executor killed
â”‚  â”œâ”€ Causa Probable: Executor memory allocation insufficient for dataset size + shuffle operations
â”‚  â””â”€ CategorÃ­a: Resource Limit (memory)
â”‚
â”œâ”€ SÃ: State tracking mechanism failed?
â”‚  â”œâ”€ SÃ­ntoma: Job state inconsistent between orchestrator and execution logs
â”‚  â”œâ”€ Causa Probable: State persistence mechanism (bookmarks/checkpoints) disabled or misconfigured
â”‚  â”œâ”€ Platform examples: Glue bookmarks, Synapse checkpoints, Data Factory pipeline tracking
â”‚  â””â”€ CategorÃ­a: State Management
â”‚
â””â”€ SÃ: Network or external dependency timeout?
   â”œâ”€ SÃ­ntoma: Connection errors to database, API, file system during job execution
   â”œâ”€ Causa Probable: External service unavailable, network latency, socket timeout
   â””â”€ CategorÃ­a: External Dependency Issue
```

**DiagnÃ³stico (agnÃ³stico):**
- Correlate orchestration state (UI console) with execution logs (CloudWatch/Log Analytics/Application Insights)
- Identify last successful operation before state divergence
- Look for error patterns in logs (not specific commands, but failure indicators)
- Compare start time, expected duration, actual duration vs configured thresholds
- Check timeout parameters in job/pipeline configuration (platform-specific location varies)

**ImplicaciÃ³n para SoluciÃ³n:**
- If timeout: Consider increasing timeout parameter (see 03-technology for options by platform)
- If memory: Consider memory allocation strategy (see 06-process for tuning runbook)
- If state tracking: Consider state management configuration (see instructions for mechanism by platform)
- If external: Verify dependency availability, consider retry/fallback strategy

---

### CategorÃ­a 2: Resource Contention (Slow Execution)

**PatrÃ³n Observable:**
- Job completes successfully but takes significantly longer than baseline
- Dataset size growth correlates with execution time growth (not linear)
- Job logs show no errors, but worker/task metrics show high contention
- Task execution times vary widely (some tasks 100x slower than others)

**Causas Posibles (Ãrbol de DiagnÃ³stico):**

```
Â¿Job execution time increases disproportionate to data growth?
â”œâ”€ SÃ: Data distribution skewed?
â”‚  â”œâ”€ SÃ­ntoma: Task execution times vary widely; some tasks 100x slower than others
â”‚  â”œâ”€ Observable: One or few partition keys concentrate majority of records
â”‚  â”œâ”€ Causa Probable: Partitioning strategy places uneven load on workers
â”‚  â””â”€ CategorÃ­a: Data Skew
â”‚
â”œâ”€ SÃ: Executor memory doing spill-to-disk?
â”‚  â”œâ”€ SÃ­ntoma: Job running but response is slow; memory pressure indicators in logs
â”‚  â”œâ”€ Observable: GC (garbage collection) time increases; shuffle operations slower
â”‚  â”œâ”€ Causa Probable: Worker memory allocation too low for shuffle/join operations
â”‚  â””â”€ CategorÃ­a: Memory Pressure
â”‚
â”œâ”€ SÃ: Shuffle partition count misaligned with dataset size?
â”‚  â”œâ”€ SÃ­ntoma: Job uses default partition count; dataset size increased
â”‚  â”œâ”€ Observable: Shuffle performance degraded; network I/O bottleneck
â”‚  â”œâ”€ Causa Probable: Partition count does not match data volume or worker count
â”‚  â””â”€ CategorÃ­a: Configuration Misalignment
â”‚
â””â”€ SÃ: Filter/aggregation applied at wrong stage?
   â”œâ”€ SÃ­ntoma: Job processes data through expensive operations before filtering
   â”œâ”€ Observable: Unnecessary broadcast of large tables; joins before filters
   â”œâ”€ Causa Probable: Operation ordering reduces efficiency (logical plan not optimized)
   â””â”€ CategorÃ­a: Query Plan Inefficiency
```

**DiagnÃ³stico (agnÃ³stico):**
- Establish baseline: expected duration, dataset size, SLA
- Measure current state: actual duration, current dataset size
- Analyze growth ratio: is performance proportional to data or superlinear?
- Examine task metrics: identify outlier tasks (much slower than others)
- Review data distribution: are there partition keys with disproportionate records?
- Assess operation sequence: are filters applied before or after expensive operations?

**ImplicaciÃ³n para SoluciÃ³n:**
- If data skew: Apply statistical distribution technique to balance load (see glue-jobs-patterns resource)
- If memory pressure: Increase executor memory or adjust shuffle fraction (see 03-technology for parameters)
- If partition misalignment: Adjust shuffle partition count based on data volume and worker count
- If plan inefficiency: Reorganize operation sequence (filters before joins/broadcasts)

---

### CategorÃ­a 3: Memory Exhaustion (OOM)

**PatrÃ³n Observable:**
- Job terminates unexpectedly with OutOfMemory error or similar heap exhaustion
- Job may appear to "hang" because OOM kills process silently
- CloudWatch may show timeout (because OOM kills executor before graceful termination)
- No normal completion message; state is FAILED

**Causas Posibles (Ãrbol de DiagnÃ³stico):**

```
Â¿Job showing memory exhaustion symptoms?
â”œâ”€ SÃ: Garbage collection unable to reclaim memory?
â”‚  â”œâ”€ SÃ­ntoma: OOM: Java heap space; high GC time in logs
â”‚  â”œâ”€ Causa Probable: Executor heap allocation too small for active operations
â”‚  â””â”€ CategorÃ­a: Insufficient Executor Heap
â”‚
â”œâ”€ SÃ: Single broadcast operation too large?
â”‚  â”œâ”€ SÃ­ntoma: OOM occurs during broadcast join; large dimension table
â”‚  â”œâ”€ Causa Probable: Broadcast table size exceeds executor memory
â”‚  â””â”€ CategorÃ­a: Broadcast Overload
â”‚
â”œâ”€ SÃ: Shuffle intermediate data larger than memory?
â”‚  â”œâ”€ SÃ­ntoma: OOM during shuffle; job processes many small records with wide transformations
â”‚  â”œâ”€ Causa Probable: Shuffle memory fraction too low; memory spill-to-disk exhausts disk
â”‚  â””â”€ CategorÃ­a: Shuffle Memory Pressure
â”‚
â””â”€ SÃ: Accumulated state in memory exceeded capacity?
   â”œâ”€ SÃ­ntoma: OOM after processing many partitions; job memory usage increases linearly
   â”œâ”€ Causa Probable: Job retaining intermediate DataFrames; missing explicit cache/drop
   â””â”€ CategorÃ­a: Accumulated State
```

**DiagnÃ³stico (agnÃ³stico):**
- Identify which operation triggered OOM (is it a join, broadcast, shuffle, aggregation?)
- Estimate data size at OOM point (input Ã— transformation factor)
- Compare against executor memory limit (see 03-technology for allocation options)
- Check if memory allocation increased linearly or had spikes
- Assess whether problem is frequency of OOM (immediate) or cumulative (after N iterations)

**ImplicaciÃ³n para SoluciÃ³n:**
- If insufficient heap: Increase executor memory allocation (vertical scaling)
- If broadcast overload: Split broadcast table or use sort-merge join instead (architectural change)
- If shuffle memory pressure: Increase shuffle memory fraction or increase number of workers (horizontal scaling)
- If accumulated state: Add explicit cache eviction or repartitioning strategy

---

### CategorÃ­a 4: State Management Failure (Bookmarks/Checkpoints/Idempotency)

**PatrÃ³n Observable:**
- Job completes but output contains duplicate records
- Running job multiple times produces duplicated data (not idempotent)
- Job logs show state management error ("bookmark not found", "checkpoint failed", "state corrupted")
- Job reprocesses data already processed in previous runs

**Ejemplos por Plataforma:**
- **AWS Glue:** Glue bookmark disabled â†’ duplicates on re-run; or bookmark corrupted in S3
- **Azure Synapse:** Spark checkpoint not enabled â†’ reprocessing entire input partition
- **Azure Data Factory:** ADFv2 watermark not incremented â†’ activity reprocesses same rows

**Causas Posibles (Ãrbol de DiagnÃ³stico):**

```
Â¿Job processing data non-idempotently (duplicates)?
â”œâ”€ SÃ: State persistence mechanism not enabled?
â”‚  â”œâ”€ SÃ­ntoma: Job reprocesses all historical data on each run
â”‚  â”œâ”€ Causa Probable: State tracking feature disabled in configuration
â”‚  â”œâ”€ Platform examples: Glue bookmarks disabled, Synapse checkpoint mode off, ADF watermark not used
â”‚  â””â”€ CategorÃ­a: Feature Configuration
â”‚
â”œâ”€ SÃ: State persistence mechanism failed silently?
â”‚  â”œâ”€ SÃ­ntoma: Job logs show checkpoint/bookmark/watermark error; output has duplicates
â”‚  â”œâ”€ Causa Probable: State persistence corrupted, lost, or not committed
â”‚  â”œâ”€ Platform examples: Glue bookmark S3 inaccessible, Synapse DPU crash during checkpoint, ADF watermark write failed
â”‚  â””â”€ CategorÃ­a: State Persistence Failure
â”‚
â”œâ”€ SÃ: Job using non-state-aware execution method?
â”‚  â”œâ”€ SÃ­ntoma: Job uses generic API without state tracking mechanism
â”‚  â”œâ”€ Causa Probable: Execution bypasses state-aware APIs (e.g., Spark read/write instead of Glue DynamicFrame; direct PySpark instead of notebook cells)
â”‚  â”œâ”€ Platform examples: Glue using Spark.read vs DynamicFrame; Synapse using direct SQL vs checkpoint-aware Spark; ADF using copy activity without watermark
â”‚  â””â”€ CategorÃ­a: API Misalignment
â”‚
â””â”€ SÃ: Output not committed atomically?
   â”œâ”€ SÃ­ntoma: Job completes but output not finalized; partial writes or temporary files remaining
   â”œâ”€ Causa Probable: Job terminated before write operations committed or finalized
   â””â”€ CategorÃ­a: Output Commitment
```

**DiagnÃ³stico (agnÃ³stico):**
- Determine if job should be idempotent (multiple runs = same output) or stateful (process only new data)
- Check if job uses state-aware APIs or mechanisms for your platform
- Verify if state persistence is enabled in job/pipeline configuration
- Examine logs for state-related errors (checkpoint failures, watermark issues, bookmark messaging)
- Compare output between sequential runs to detect duplication patterns

**ImplicaciÃ³n para SoluciÃ³n:**
- If feature disabled: Enable state persistence in job configuration (see 03-technology for platform-specific setup)
- If state persistence failed: Investigate state storage location (S3 bookmarks, Synapse checkpoint dir, ADF watermark table); may need reset
- If API misaligned: Use platform-native state-aware APIs (Glue DynamicFrame, Synapse checkpoint context, ADF watermark pattern)
- If output not committed: Ensure job completes gracefully; configure atomic write operations (see 06-process for platform-specific patterns)

---

## ğŸ” SECUENCIA DE TRIAGE AGNÃ“STICA

### Fase 0: RecopilaciÃ³n de Observables
1. **Estado observado:** Â¿QuÃ© reporta el job/activity state en UI (Glue console, Synapse Studio, Data Factory Portal)?
2. **SÃ­ntoma primario:** Describe lo que observas (colgado, lento, error, duplicados)
3. **DuraciÃ³n contextual:** Â¿DuraciÃ³n actual vs SLA establecido?
4. **Cambio reciente:** Â¿Datos aumentaron? Â¿ConfiguraciÃ³n cambiÃ³? Â¿CÃ³digo actualizado?
5. **Frecuencia:** Â¿Ocurre siempre o intermitentemente?
6. **Plataforma & ejecutor:** Â¿Glue, Synapse Spark, Data Factory activity, etc.?

### Fase 1: ClasificaciÃ³n de SÃ­ntoma
```
SÃ­ntoma observado â†’ CategorÃ­a probable:

"Job stuck en RUNNING, state discrepancy"
  â†’ CategorÃ­a 1: Discrepancia de Estado

"Job completÃ³ pero tardÃ³ 4x"
  â†’ CategorÃ­a 2: Resource Contention (slowness)

"Job terminÃ³ con OutOfMemory"
  â†’ CategorÃ­a 3: Memory Exhaustion

"Output tiene duplicados post-run"
  â†’ CategorÃ­a 4: State Management Failure
```

### Fase 2: Ãrbol de DiagnÃ³stico
- Follow the decision tree in corresponding category
- Eliminate causes progressively based on observable evidence
- Narrow to most probable cause category

---

### âš¡ FASE 2.5: NEXT STEP Actions (What to Do After Diagnosis)

**After identifying the category, follow EXACTLY these next steps:**

#### âœ… If CategorÃ­a 1: State Discrepancy

**Next step workflow:**
1. **Collect evidence** (5 min):
   - Take screenshot of job state in UI (RUNNING)
   - Export logs from CloudWatch/Log Analytics for last 30 minutes
   - Note exact timestamp of state divergence

2. **Self-serve fix options:**
   - IF timeout issue: Increase job timeout parameter (see 03-technology) â†’ Redeploy â†’ Rerun
   - IF memory issue: Manually scale up executor memory (vertical scaling) â†’ Test in staging â†’ Redeploy
   - IF network/external: Verify external service availability (DB, API) â†’ Check connectivity â†’ Rerun

3. **Escalate if (after 30 min investigation):**
   - State still discrepant after fix attempts
   - Infrastructure issue suspected (S3 inaccessible, network firewall)
   - â†’ **Escalate to Data Platform / SRE** with: logs + fix attempts + timeline

---

#### âš¡ If CategorÃ­a 2: Resource Contention (Slow)

**Next step workflow:**
1. **Quick diagnosis** (15 min):
   - Is this data skew (fixable by salting) or resource ceiling (needs redesign)?
   - Use: "CuÃ¡ndo hacer tuning vs redesign" section in performance-optimization.md

2. **Self-serve fix (if tuning):**
   - Apply salting to hot keys OR increase executor memory OR add workers
   - Test in staging before prod
   - Reference: glue-jobs-patterns.md for code examples
   - Rerun and monitor metrics

3. **Escalate if (after 1 hour tuning):**
   - Performance doesn't improve despite tuning
   - Max resources still insufficient
   - â†’ **Escalate to Data Platform + Data Architect** with: metrics + tuning attempts + redesign hypothesis

---

#### ğŸ”´ If CategorÃ­a 3: Memory Exhaustion (OOM)

**Next step workflow:**
1. **Immediate action** (5 min):
   - This is **BLOCKING** â€” job cannot complete; must act now
   - Identify WHERE OOM occurred (in which operation: join, broadcast, shuffle, aggregation?)

2. **Self-serve fix:**
   - If broadcast overload: Use sort-merge join instead OR split large table
   - If executor heap insufficient: Increase executor memory (see 03-technology)
   - If shuffle spill: Increase shuffle memory fraction
   - Redeploy and rerun immediately

3. **Escalate if (after first fix attempt fails):**
   - OOM persists even with max memory
   - â†’ **Escalate to Data Platform / SRE** with: exact OOM message + memory config + job logs
   - **SRE action:** Investigate memory limit, consider job redesign (streaming vs batch, map-reduce structure)

---

#### ğŸ”„ If CategorÃ­a 4: State Management (Duplicates)

**Next step workflow:**
1. **Verify the problem** (10 min):
   - Count existing rows in output table BEFORE rerun
   - Count rows after rerun
   - Difference = duplicates (YES, escalate; NO, was false alarm)

2. **Self-serve fix:**
   - Enable state persistence in job config:
     - **AWS Glue:** Enable job bookmarks or implement checkpoint logic
     - **Azure Synapse:** Enable checkpoint context in Spark notebook
     - **Azure Data Factory:** Implement watermark pattern (metadata table tracking)
   - Reference: 03-technology â†’ feature setup section
   - Optionally: Manual state reset if bookmarks corrupted
   - Rerun and verify output deduplication

3. **Escalate if (after state reset fails):**
   - Duplicates persist OR state reset error occurred
   - â†’ **Escalate to Data Platform** with: duplicate row sample + state config + error message
   - **Data Platform action:** Investigate state storage location (S3 bookmarks, table corruption)

---

### Fase 3: RecomendaciÃ³n de AcciÃ³n
- Reference 03-technology for tuning parameters
- Reference 06-process for operational runbook sequence
- Reference glue-jobs-patterns resource for implementation patterns
- Consider escalation to incident-triage prompt if root cause unclear

### Fase 4: ValidaciÃ³n Post-Remedy
- Establish new baseline (expected durations, resource metrics)
- Run job and monitor state transitions
- Verify output correctness (no duplicates, completeness)
- Confirm logs show no error patterns
- Document findings for future troubleshooting reference

---

## PATRONES DE MITIGACIÃ“N GENÃ‰RICOS

### PatrÃ³n: Balancing Skewed Load
**Aplicable a:** CategorÃ­a 2 (Data Skew)
**Concepto:** Distribute disproportionately-loaded partition keys across multiple workers.
**Estrategia:** Add statistical factor to transform hot keys into multiple independent partitions.
**Referencia:** glue-jobs-patterns â†’ Section on "Salting Strategy"

### PatrÃ³n: Hierarchical Memory Management
**Aplicable a:** CategorÃ­a 3 (Memory Exhaustion)
**Concepto:** Allocate memory strategically between heap, shuffle operations, and storage.
**Estrategia:** See 03-technology for memory parameters; adjust shuffle fraction vs executor heap ratio.
**Referencia:** 06-process â†’ Runbook Section 5.4.2 "Memory Tuning Sequence"

### PatrÃ³n: Stateful Job Design
**Aplicable a:** CategorÃ­a 4 (State Management)
**Concepto:** Enable state persistence mechanism to track processed records and avoid reprocessing.
**Estrategia (por plataforma):**
  - **AWS Glue:** Use Glue-native APIs (DynamicFrame) with bookmarks enabled; verify atomic write
  - **Azure Synapse:** Configure Spark job checkpoint context; use notebook idempotent patterns
  - **Azure Data Factory:** Implement watermark pattern with metadata table tracking
**Referencia:** 03-technology â†’ Feature configuration section; glue-jobs-patterns â†’ "Idempotent Design" (translatable across platforms)

### PatrÃ³n: Resource Right-Sizing
**Aplicable a:** CategorÃ­as 1, 2, 3 (Multiple)
**Concepto:** Match worker count and memory allocation to dataset size and operation complexity.
**Estrategia:** Establish baseline metrics; scale incrementally; monitor return on investment.
**Referencia:** 03-technology â†’ "Capacity Planning"; 06-process â†’ "Scaling Decisions"

---

## REFERENCIAS RELACIONADAS

- **Prompt (Tier 2):** [incident-triage.md](incident-triage.md) â€” Escalation pathway if categorization unclear (covers operational incidents across platforms)
- **Prompt (Tier 2):** [performance-optimization.md](performance-optimization.md) â€” Deep analysis for Category 2 (slowness diagnosis), applicable to Spark/Flink-based systems
- **Prompt (Tier 2):** [pipeline-orchestration-design.md](./pipeline-orchestration-design.md) â€” If job failures are orchestration-related (retry policy, dependency, scheduling), validate pipeline configuration
- **Instrucciones (06-process):** [modular/06-process.md](../../instructions_or_rules/data-engineering/modular/06-process.md) â€” Operational runbooks + memory tuning sequences by platform
- **Instrucciones (03-technology):** [modular/03-technology.md](../../instructions_or_rules/data-engineering/modular/03-technology.md) â€” Job parameters, capacity options, state management config by platform (Glue, Synapse, etc.)
- **Resource (Tier 2):** [aws-azure-data-services.md](../../resources/data-engineering/aws-azure-data-services.md) â€” Platform-specific tuning and troubleshooting patterns (AWS vs Azure); use for translating Glue concepts to Synapse/Databricks
- **Resource (Tier 3):** [glue-jobs-patterns.md](../../resources/data-engineering/glue-jobs-patterns.md) â€” AWS Glue-specific patterns (skew mitigation, idempotence, bookmarks). **Translation guide:** Replace Glue bookmarks â†’ Synapse checkpoints / Databricks Unity Catalog; Glue DynamicFrame â†’ Spark DataFrames with merge logic; S3 â†’ ADLS Gen2. See aws-azure-data-services.md Section 3 for feature mapping.
