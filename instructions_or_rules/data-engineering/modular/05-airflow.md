# Airflow & MWAA: Implementation, Deployment y Operaciones

GuÃ­a completa para implementar, desplegar y operar Airflow en AWS Managed Workflows (MWAA).

---

## 1. MWAA vs Self-Hosted Airflow

### Comparativa

| Aspecto | MWAA (AWS Managed) | Self-Hosted | Usar MWAA si... |
|--------|-------------------|-------------|-----------------|
| **Setup** | Completo en AWS | Manual (EC2/ECS) | No quieres ops de infraestructura |
| **Scaling** | AutomÃ¡tico | Manual | Necesitas auto-scaling |
| **Updates** | AWS los maneja | Manual | Quieres Ãºltimo Airflow pero sin mantenimiento |
| **Costo** | ~$0.48/hour + storage | Flexible | Cargas pequeÃ±as-medianas (<10 DAGs) |
| **IntegraciÃ³n AWS** | Nativa | Via providers | Usas muchos servicios AWS |
| **Control** | Limitado (kubelet, security groups) | Completo | Necesitas personalizaciÃ³n extrema |

**RecomendaciÃ³n:** Usa MWAA para la mayorÃ­a de equipos Pragma.

---

## 2. Setup Inicial de MWAA en AWS

### Paso 1: Crear Bucket S3 para DAGs y logs

```bash
aws s3api create-bucket \
    --bucket pragma-airflow-env \
    --region us-east-1

# Estructura esperada
s3://pragma-airflow-env/
â”œâ”€â”€ dags/           # AquÃ­ van los DAGs Python
â”œâ”€â”€ plugins/        # Custom operators, hooks
â”œâ”€â”€ requirements.txt # Dependencias Python
â””â”€â”€ logs/           # Logs (MWAA escribe aquÃ­)
```

### Paso 2: Crear IAM Role para MWAA

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "airflow:PublishMetrics",
      "Resource": "arn:aws:airflow:*:*:*"
    },
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "airflow.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::pragma-airflow-env/*",
        "arn:aws:s3:::pragma-airflow-env"
      ]
    },
    {
      "Effect": "Allow",
      "Action": [
        "logs:CreateLogStream",
        "logs:CreateLogGroup",
        "logs:PutLogEvents",
        "logs:GetLogEvents"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject"
      ],
      "Resource": "arn:aws:s3:::pragma-airflow-env/logs/*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "redshift:DescribeClusters",
        "redshift-data:ExecuteStatement",
        "redshift-data:DescribeStatement",
        "redshift-data:GetStatementResult"
      ],
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "glue:*"
      ],
      "Resource": "*"
    }
  ]
}
```

### Paso 3: Crear Environment en MWAA

Via AWS Console o CLI:

```bash
aws mwaa create-environment \
    --name pragma-airflow-prod \
    --airflow-version 2.4.3 \
    --environment-class mw1.small \
    --max-workers 10 \
    --dag-s3-path s3://pragma-airflow-env/dags/ \
    --plugins-s3-path s3://pragma-airflow-env/plugins/ \
    --requirements-s3-path s3://pragma-airflow-env/requirements.txt \
    --execution-role-arn arn:aws:iam::ACCOUNT:role/MWAAExecutionRole \
    --region us-east-1 \
    --tags Environment=production Team=data-engineering
```

**ParÃ¡metros:**
- **airflow-version:** 2.4.3 o superior (soporte a Airflow 2.x)
- **environment-class:** mw1.small (dev), mw1.medium (staging), mw1.large (prod)
- **max-workers:** 10-25 para prod

---

## 3. Estructura de Proyecto Recomendada

```
pragma-airflow/
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ data_engineering/
â”‚   â”‚   â”œâ”€â”€ ingest_apis_to_s3.py
â”‚   â”‚   â”œâ”€â”€ transform_batch.py
â”‚   â”‚   â””â”€â”€ validate_quality.py
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ analytics_dashboard.py
â”‚   â”‚   â””â”€â”€ ml_predictions.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”œâ”€â”€ custom_s3_operator.py
â”‚   â”‚   â””â”€â”€ redshift_operator.py
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ pragmadb_hook.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_extract_dag.py
â”‚   â”‚   â””â”€â”€ test_transformations.py
â”‚   â””â”€â”€ integration/
â”‚       â””â”€â”€ test_e2e_pipeline.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ deploy.sh       # Deploy DAGs a S3/MWAA
â”‚   â”œâ”€â”€ sync_requirements.sh
â”‚   â””â”€â”€ test_local.sh   # Test con docker-compose
â”œâ”€â”€ requirements.txt    # Python deps
â”œâ”€â”€ docker-compose.yml  # Local Airflow development
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ðŸ“Œ **En S3:**
s3://pragma-airflow-env/
â”œâ”€â”€ dags/               â† sync desde dags/
â”œâ”€â”€ plugins/            â† sync desde plugins/
â””â”€â”€ requirements.txt    â† sync desde ./requirements.txt
```

---

## 4. Deployment (CI/CD con GitHub Actions)

### Script: `scripts/deploy.sh`

```bash
#!/bin/bash

BUCKET="pragma-airflow-env"
REGION="us-east-1"

echo "ðŸ“¦ Syncing DAGs to S3..."
aws s3 sync ./dags s3://${BUCKET}/dags/ --delete --region ${REGION}

echo "ðŸ“¦ Syncing plugins to S3..."
aws s3 sync ./plugins s3://${BUCKET}/plugins/ --delete --region ${REGION}

echo "ðŸ“¦ Syncing requirements.txt to S3..."
aws s3 cp requirements.txt s3://${BUCKET}/requirements.txt --region ${REGION}

echo "âœ… Deployment complete. DAG parsing will trigger in ~1 min."
```

### GitHub Actions Workflow: `.github/workflows/deploy-airflow.yml`

```yaml
name: Deploy Airflow DAGs

on:
  push:
    branches: [ main, develop ]
    paths:
      - dags/**
      - plugins/**
      - requirements.txt

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          pip install apache-airflow[amazon] pandas pyarrow
      
      - name: Validate DAGs
        run: |
          airflow dags validate
      
      - name: Run unit tests
        run: |
          pytest tests/unit/ -v

  deploy:
    runs-on: ubuntu-latest
    needs: validate
    if: github.ref == 'refs/heads/main'
    steps:
      - uses: actions/checkout@v3
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::ACCOUNT:role/GitHubActionsRole
          aws-region: us-east-1
      
      - name: Deploy to MWAA
        run: bash scripts/deploy.sh
```

---

## 5. Variables, Connections y Secrets

### Crear Variables en MWAA

**Via Airflow UI:**
1. Admin â†’ Variables
2. Click "Create"
3. Key: `ENVIRONMENT`, Value: `production`
4. Save

**Via CLI:**
```bash
airflow variables set ENVIRONMENT production
airflow variables set SLACK_WEBHOOK https://hooks.slack.com/...
```

**Via AWS CLI (sin UI):**
```bash
aws mwaa create-cli-token --name pragma-airflow-prod --region us-east-1
# Extrae token...
airflow variables set ENVIRONMENT production  # Dentro del container
```

### Crear Connections

**Via Airflow UI:**
1. Admin â†’ Connections
2. Click "Create"
3. Conn ID: `redshift_default`
4. Conn Type: `postgres` (Redshift es compatible)
5. Host: `redshift-cluster.xyz.us-east-1.redshift.amazonaws.com`
6. Port: `5439`
7. Login: `awsuser`
8. Password: `xxxxx`
9. Database: `dev`
10. Save

**Uso en DAG:**
```python
from airflow.providers.postgres.operators.postgres import PostgresOperator

redshift_query = PostgresOperator(
    task_id='redshift_query',
    postgres_conn_id='redshift_default',
    sql='SELECT * FROM sales LIMIT 10;',
    dag=dag,
)
```

---

## 6. Monitoreo y Alertas

### CloudWatch Alarms

```bash
aws cloudwatch put-metric-alarm \
    --alarm-name airflow-dag-failure \
    --alarm-description "Alert when DAG fails" \
    --metric-name DAGRunFailure \
    --namespace AWS/MWAA \
    --statistic Sum \
    --period 300 \
    --threshold 1 \
    --comparison-operator GreaterThanOrEqualToThreshold \
    --alarm-actions arn:aws:sns:us-east-1:ACCOUNT:airflow-alerts
```

### Slack Notifications

```python
from airflow.providers.slack.operators.slack_webhook import SlackWebhookOperator
from airflow.utils.dates import days_ago

def on_dag_fail(context):
    """Send failure alert to Slack."""
    dag_run = context['dag_run']
    slack_msg = f"""
    âŒ DAG Failed: {dag_run.dag_id}
    Try: {context['task_instance'].try_number}
    Time: {dag_run.execution_date}
    """
    return slack_msg

dag = DAG(
    dag_id='data_engineering.analytics',
    on_failure_callback=on_dag_fail,
    start_date=days_ago(1),
)
```

---

## 7. Troubleshooting MWAA

| Problema | SÃ­ntoma | SoluciÃ³n |
|----------|---------|----------|
| **DAG no aparece** | Airflow UI vacÃ­o | Validar DAG; revisar S3 permisos |
| **Task fails con "access denied"** | Error en task logs | Revisar IAM role; aÃ±adir permisos S3/Glue |
| **Memory issues** | Job mata con OOM | Aumentar environment-class o max-workers |
| **Airflow UI lento** | UI tarda >10 seg | Reducir nÃºmero de DAGs; aÃ±adir DB read replicas |
| **Secrets no se encuentran** | KeyError en Variable.get() | Verificar nombre exacto en Airflow UI |

---

## 8. Best Practices Operacionales

### Backup y Disaster Recovery

```bash
# Backup metadata DB
aws rds create-db-snapshot \
    --db-instance-identifier airflow \
    --db-snapshot-identifier airflow-backup-$(date +%Y%m%d)

# Exportar DAGs
aws s3 sync s3://pragma-airflow-env/dags/ ./dags-backup/
```

### RotaciÃ³n de Logs

```python
# En MWAA, logs se envÃ­an a CloudWatch y S3
# Configurar retention en CloudWatch
aws logs put-retention-policy \
    --log-group-name /aws/airflow/dag-logs \
    --retention-in-days 30
```

### Upgrades de Airflow

1. Airflow anuncia nuevas versiones; AWS las aÃ±ade a MWAA despuÃ©s de 1-2 meses
2. Testa en staging primero
3. Programa upgrade en horario bajo
4. MWAA tarda ~5-10 min en actualizar

---

---

## 9. LibrerÃ­as y Operadores Custom Reutilizables

### ðŸ“¦ Repositorio: ciencia-datos-datos-lib-py-operators

**PropÃ³sito:** LibrerÃ­a de operadores Airflow custom tuned para pipelines Pragma.

**Operadores disponibles:**
- `S3MultipartCopyOperator`: Copia archivos S3 grandes (soporta multipart â‰¤5GB o >5GB)
- `FileFerryOperator`: Transferencias S3â†”SFTP vÃ­a AWS Lambda
- `FileFerryTransferSensor`: Monitor de estado de transferencias
- `FileFerryCompletionSensor`: Espera estado COMPLETED/PARTIALLY_COMPLETED/FAILED
- `FileFerryFailureSensor`: DetecciÃ³n de transferencias fallidas

**Repo:** https://github.com/carlosguzmanbaq/ciencia-datos-datos-lib-py-operators

**Casos de uso:**
- Pipelines que requieren copias S3 de archivos >5GB
- Integraciones S3â†”SFTP sin escribir cÃ³digo custom
- Monitoreo de transferencias en tiempo real

**InstalaciÃ³n:**
```bash
pip install git+https://github.com/carlosguzmanbaq/ciencia-datos-datos-lib-py-operators.git
```

**Ejemplo DAG:**
```python
from airflow_operators.s3 import S3MultipartCopyOperator
from airflow_operators.fileferry import FileFerryOperator, FileFerryCompletionSensor

# Copiar archivo grande
copy_task = S3MultipartCopyOperator(
    task_id='copy_large_file',
    source_s3_key='s3://bucket-source/large-file.parquet',
    destination_s3_key='s3://bucket-dest/large-file.parquet',
    multipart_threshold=5 * 1024**3,  # 5GB
)

# Transferir a SFTP
transfer = FileFerryOperator(
    task_id='transfer_to_sftp',
    operation='upload',
    source_s3_path='s3://bucket/data/',
    target_sftp_path='/remote/data/',
)

# Esperar confirmaciÃ³n
wait_completion = FileFerryCompletionSensor(
    task_id='wait_transfer_done',
    transfer_id='{{ ti.xcom_pull(task_ids="transfer_to_sftp", key="transfer_id") }}',
    poke_interval=30,
)

copy_task >> transfer >> wait_completion
```

### ðŸ“¦ Repositorio: ciencia-datos-datos-lib-py-fileferry (Dependencia)

**PropÃ³sito:** Servicio AWS Lambda que abstrae transferencias S3â†”SFTP usando AWS Transfer Family.

**Funcionalidades:**
- Upload (S3â†’SFTP), Download (SFTPâ†’S3), Delete, List, Get Status
- Batch orchestration, throttle control, session management
- Comprehensive error codes y retry handling

**Repo:** https://github.com/jersonferrerm/ciencia-datos-datos-lib-py-fileferry

**Caso de uso:** Backend de `FileFerryOperator`. Puedes invocar directamente si necesitas APIs REST.

**API REST Example:**
```bash
# Upload a S3â†’SFTP
curl -X POST https://lambda-endpoint/transfer \
  -H "Content-Type: application/json" \
  -d '{
    "operation": "upload",
    "source_s3_path": "s3://bucket/data.csv",
    "target_sftp_path": "/sftp/data.csv",
    "sftp_connection_id": "prod-sftp-conn"
  }'
```

---

### REFERENCIAS RELACIONADAS

- **Prompt:** `prompts/data-engineering/airflow-dag-design.md` (ValidaciÃ³n de DAGs)
- **Resource:** `resources/data-engineering/airflow-best-practices.md` (Patrones, logging, testing)
- **Instrucciones:** `instructions_or_rules/data-engineering/modular/02-guidelines.md` (Error handling, logging)
- **Resource:** `resources/data-engineering/aws-azure-data-services.md` (AWS services mapping)
- **ðŸ”— Externos:** `ciencia-datos-datos-lib-py-operators` (Operadores custom), `ciencia-datos-datos-lib-py-fileferry` (Backend Lambda)
